# Whisper Fine-tuning Experiment Results

## Environment
- Platform: Lambda Labs A10 GPU Instance
- GPU Memory: 24GB
- Python Version: 3.9
- Base Model: openai/whisper-base

## Dataset Overview
- Source: pauljunsukhan/throatmic_codered
- Total Examples: 180
- Training Split: 144 examples (80%)
- Test Split: 36 examples (20%)
- Audio Characteristics:
  - Sampling Rate: 16kHz
  - Duration: ~10 seconds per clip
  - Format: Throat microphone recordings

## Experiment Results

### Baseline Performance
- Initial WER: 101.43%
- Example Predictions:
```
Reference: "One aspect of this is the lack of "high rise" hotels on the island."
Prediction: "Well, I'm expecting this is the last of high rise hotels on the island."

Reference: "Subsequently, he accused his allies of spying on him and working to harm Samoobrona."
Prediction: "Subscribe on the QSS and on the S1 website."
```

### Training Process
- Training Duration: 43 minutes
- Total Steps: 1000
- Epochs: 111.11
- Batch Size: 16
- Learning Rate: 1e-5 â†’ 4.2e-08 (linear decay)

#### Loss Progression
1. Initial Loss: 7.8466
2. Early Training: Rapid decrease
   - Step 100: 3.1987
   - Step 200: 0.6866
   - Step 300: 0.129
3. Final Loss: 0.0003

### Fine-tuned Performance
- Final WER: 59.76%
- Absolute Improvement: 41.67%
- Example Predictions:
```
Reference: "One aspect of this is the lack of "high rise" hotels on the island."
Prediction: "One I have been expecting this is the lack of high rise hotels on the island."

Reference: "When this is diagonalized, the eigenvectors are chosen as the expansion coefficients."
Prediction: "When this is diagnosed, the eye concaptors are chosen as expansion-coded patients."
```

## Analysis

### Current Limitations
1. **Dataset Size**
   - Current: 180 examples
   - Industry Minimum: 1,000-2,000 examples
   - Recommended: 10,000+ examples
   - Our dataset is only ~10% of minimum recommended size

2. **Model Behavior**
   - High WER even after fine-tuning
   - Hallucination-like behavior
   - Struggles with:
     - Complex terminology
     - Proper nouns
     - Sentence structure

3. **Training Dynamics**
   - Very rapid initial loss decrease suggests large parameter updates
   - Potential overfitting despite regularization attempts
   - Limited data prevents proper generalization

## Recommendations

### 1. Data Collection Priority
- **Target Size**: 
  - Minimum: 1,000 examples
  - Ideal: 2,000-5,000 examples
- **Diversity Needs**:
  - Multiple speakers
  - Various sentence structures
  - Technical terms and proper nouns
  - Different speaking speeds

### 2. Future Model Improvements
(To be implemented after data collection)

1. **Learning Rate Optimization**
   - Reduce from 1e-5 to 5e-6
   - Increase warmup steps to 100
   - Implement smoother learning rate decay

2. **Batch Size Adjustment**
   - Increase from 16 to 32/64
   - Add gradient accumulation
   - Better utilize A10's 24GB memory

3. **Regularization Suite**
   - Weight decay: 0.01
   - Dropout: 0.1
   - Label smoothing: 0.1
   - Early stopping
   - Gradient clipping

## Next Steps
1. Focus on data collection until reaching minimum 1,000 examples
2. Ensure data quality and diversity during collection
3. Re-run baseline with larger dataset
4. Implement model improvements in phases
5. Document comparative results

## Technical Notes
- All code and configurations are stored in `whisper_experiment.py`
- Training logs and model checkpoints in `whisper_finetuned/`
- Raw metrics stored in `experiment_results.txt` 